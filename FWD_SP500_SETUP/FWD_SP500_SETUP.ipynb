{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "raknyksivw7eihv6sqfe",
   "authorId": "4547991016428",
   "authorName": "FWDDEMO",
   "authorEmail": "greg.wolszczak@sbi-group.com",
   "sessionId": "b1e4595e-d101-4e88-abde-a6bd606bc43a",
   "lastEditTime": 1745835046592
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1411983-ffe5-46e8-bd4b-5fa3fce60379",
   "metadata": {
    "name": "Step1",
    "collapsed": false
   },
   "source": "## Setup the environment (database, schema, stage, warehouse, etc)\n+ Copy the below setup into Snowflake Worksheet to initiated the environment "
  },
  {
   "cell_type": "code",
   "id": "5360b613-f017-4464-adbb-6b2669a1fc3a",
   "metadata": {
    "language": "sql",
    "name": "Setup_Enviornment"
   },
   "outputs": [],
   "source": "-- to enable cortex on the unsported region\nALTER ACCOUNT SET CORTEX_ENABLED_CROSS_REGION = 'AWS_US'; \n\n-- Create FWDDEMO database\nCREATE OR REPLACE DATABASE FWDDEMO;\n\n-- Create schema\nCREATE OR REPLACE SCHEMA DEMO;\n\nCREATE STAGE STG_DEMO;\n\nCREATE OR REPLACE WAREHOUSE CORTEX_ANALYST_WH;\nWAREHOUSE_SIZE = 'LARGE'  \nAUTO_SUSPEND = 60;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13e59fc3-9080-40fa-adc7-279bd62813a6",
   "metadata": {
    "name": "Step2",
    "collapsed": false
   },
   "source": "## Create view tables for SP_500 and SP_500_VISITS\n\n- To create the SP_500 view table:\n    - Upload the SP_500 file (referential data) into FWDDEMO.PUBLIC\n    - Upload the S&P_500_Finance_Info file (referential data) into FWDDEMO.PUBLIC\n- To create the SP_500_VISITS view table:\n    - Go to Snowflake Marketplace and search this data **\"S & P 500 by domain and aggregated by tickers (Sample)\"**\n    - Click get data and create a new databset `SP_500_BY_DOMAIN` to store the new dataset from marketplace\n"
  },
  {
   "cell_type": "code",
   "id": "7750194b-80b5-4300-ac4f-67002dcf6e11",
   "metadata": {
    "language": "sql",
    "name": "Create_View_Tables"
   },
   "outputs": [],
   "source": "-- Create SP_500 view table\ncreate or replace view FWDDEMO.PUBLIC.\"SP_500_UPDATE\" as\nselect distinct \n    a.SYMBOL as TICKER,  \n    COALESCE(a.SECURITY, b.NAME) AS NAME,\n    b.SECTOR, \n    a.GICS_SUB_INDUSTRY as INDUSTRY, \n    a.HEADQUARTERS_LOCATION as HEADQUARTERS, \n    a.DATE_ADDED, \n    a.CIK, \n    a.FOUNDED,\n    b.PRICE, \n    b.PRICE_EARNINGS, \n    b.DIVIDEND_YIELD, \n    b.EARNING_SHARE as EARNINGS_SHARE, \n    b.\"52_Week_Low\" as \"LOW_52_WEEKS\", \n    b.\"52_Week_High\" as \"HIGH_52_WEEKS\", \n    b.MARKET_CAP, \n    b.EBITDA, \n    b.\"Price_Sales\" as \"PRICE_SALES\", \n    b.\"Price_Book\" as \"PRICE_BOOK\", \n    b.SEC_FILINGS\nfrom FWDDEMO.PUBLIC.\"SP_500\" a\nleft join FWDDEMO.PUBLIC.\"S&P_500_Finance_Info\" b ON a.SYMBOL = b.SYMBOL;\n\n--Create SP_500_VISITS view table\n\ncreate or replace view FWDDEMO.PUBLIC.\"SP_500_VISITS\" as\nselect\nTICKER,\nCOMPANY_NAME,  \nDOMAIN, \nDATE::DATE as DATE, \nCOUNTRY, \nDESKTOP_VISITS::FLOAT as DESKTOP_VISITS, \nDESKTOP_BOUNCE_RATE::FLOAT as DESKTOP_BOUNCE_RATE, \nDESKTOP_AVG_VISIT_DURATION::FLOAT as DESKTOP_AVG_VISIT_DURATION, \nDESKTOP_PAGES_PER_VISIT::FLOAT as DESKTOP_PAGES_PER_VISIT,\nMOBILE_VISITS::FLOAT as MOBILE_VISITS, \nMOBILE_BOUNCE_RATE::FLOAT as MOBILE_BOUNCE_RATE, \nMOBILE_AVG_VISIT_DURATION::FLOAT as MOBILE_AVG_VISIT_DURATION, \nMOBILE_PAGES_PER_VISIT::FLOAT as MOBILE_PAGES_PER_VISIT, \nTOTAL_VISITS::FLOAT as TOTAL_VISITS, \nTOTAL_BOUNCE_RATE::FLOAT as TOTAL_BOUNCE_RATE, \nTOTAL_AVG_VISIT_DURATION::FLOAT as TOTAL_AVG_VISIT_DURATION, \nTOTAL_PAGES_PER_VISIT::FLOAT as TOTAL_PAGES_PER_VISIT\nfrom SP_500_BY_DOMAIN.DATAFEEDS.SP_500;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d2279fb-ed5a-4e84-9977-f71f61b5bf78",
   "metadata": {
    "name": "Step3",
    "collapsed": false
   },
   "source": "## S&P 500 Fuzzy Matching\n\n### ðŸ“¦ Package Overview\n\nThis package enables interactive data apps using **Streamlit**, with data processing via **Pandas** and **Snowflake Snowpark**, and fuzzy matching powered by **RapidFuzz**.\n\n#### ðŸ”§ Imports:\n- `pandas` â€“ for data manipulation\n- `streamlit` â€“ for building the app interface\n- `snowflake.snowpark.context` â€“ to connect and run queries in Snowflake\n- `rapidfuzz.fuzz` â€“ for efficient string similarity matching\n\n#### Fuzzy Matching \n+ Using the security column (company name) to perform fuzzy matching with the article content\n+ For this DEMO, we define the threshold = 85"
  },
  {
   "cell_type": "code",
   "id": "3be443b1-14ae-4257-9df4-3bbff782c67c",
   "metadata": {
    "language": "python",
    "name": "SP_500_Fuzzy_Matching"
   },
   "outputs": [],
   "source": "import pandas as pd\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom rapidfuzz import fuzz\n\n# Get Snowflake session\nsession = st.connection('snowflake').session()\n\n# Fetch article data\ndef fetch_data_from_snowflake():\n    snowflake_table = session.table(\"FWDDEMO.PUBLIC.ARTICLE_CONTENT_SCRAPED\")\n    return snowflake_table.to_pandas()\n\n# Fetch S&P 500 SECURITY and SYMBOL\ndef fetch_sp500_data():\n    snowflake_table = session.table(\"FWDDEMO.PUBLIC.SP_500\")\n    return snowflake_table.to_pandas()[[\"SECURITY\", \"SYMBOL\"]].dropna()\n\n# Fuzzy match based on SECURITY, return all matching SYMBOLs\ndef fuzzy_match_symbols_only(content, sp500_df, threshold=85):\n    content_lower = content.lower()\n    symbols = []\n\n    for _, row in sp500_df.iterrows():\n        name = row[\"SECURITY\"]\n        symbol = row[\"SYMBOL\"]\n        score = fuzz.partial_ratio(name.lower(), content_lower)\n\n        if score >= threshold:\n            symbols.append(symbol)\n\n    return list(set(symbols)) if symbols else None\n\n# --- Process Data ---\ndata = fetch_data_from_snowflake()\nsp500_df = fetch_sp500_data()\n\n# Apply fuzzy matching to content and return only matching symbols\ndata[\"SP_500_Symbol\"] = data[\"content_scraped\"].apply(\n    lambda x: fuzzy_match_symbols_only(x, sp500_df) if pd.notnull(x) else None\n)\n\n# --- Display in Streamlit ---\nst.dataframe(data)\n\n# --- Save back to Snowflake ---\nsnowpark_df = session.create_dataframe(data)\nsnowpark_df.write.mode(\"overwrite\").save_as_table(\"FWDDEMO.PUBLIC.ArticleContentScraped_Matched\")\n\nst.success(\"âœ… Fuzzy-matched SYMBOLs saved to ArticleContentScraped_Matched!\")\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30e1ed37-a8cf-4273-9d1b-ffc0acf45859",
   "metadata": {
    "name": "Step4",
    "collapsed": false
   },
   "source": "### Building a sentiment analysis for the article content using VADER_LEXICON dictionary \n\n#### ðŸ”§ Imports:\n- `regex` â€“ work with Regular Expressions\n\n#### Sentiment Labeling:\n\n+ Scores â‰¤ -0.05 are labeled as \"Negative\"\n+ Scores â‰¥ 0.05 are labeled as \"Positive\"\n+ Scores between -0.05 and 0.05 are labeled as \"Neutral\""
  },
  {
   "cell_type": "code",
   "id": "d180f87e-3805-436d-be6f-1fb00d01cd9e",
   "metadata": {
    "language": "python",
    "name": "Sentiment_Analysis"
   },
   "outputs": [],
   "source": "import streamlit as st\nfrom snowflake.snowpark import Session\nimport pandas as pd\nimport re\n\n# Step 1: Get Snowflake session\nsession = st.connection('snowflake').session()\n\n# Check table structure first\ntable_columns = session.sql(\"SELECT * FROM FWDDEMO.PUBLIC.ARTICLECONTENTSCRAPED_MATCHED LIMIT 1\").schema.names\n#st.write(\"Available columns:\", table_columns)\n\n# Step 2: Load vader_lexicon from Snowflake table\nlexicon_df = session.sql(\"SELECT $1, $2 FROM FWDDEMO.PUBLIC.VADER_LEXICON\").to_pandas()\n\n# Optional: Display loaded lexicon\n#st.write(\"Lexicon Preview\", lexicon_df.head())\n\n# Step 3: Build lexicon dictionary\nlexicon_dict = dict(zip(lexicon_df['$1'], lexicon_df['$2']))\n\n# Step 4: Define a simple sentiment analyzer using the lexicon\nclass CustomVaderSentimentAnalyzer:\n    def __init__(self, lexicon_dict):\n        self.lexicon = lexicon_dict\n        \n    def polarity_scores(self, text):\n        \"\"\"\n        Return a float for sentiment strength based on the lexicon.\n        Positive values are positive sentiment, negative value are negative sentiment.\n        \"\"\"\n        # Handle None or empty text\n        if text is None or not isinstance(text, str):\n            return {'compound': 0, 'pos': 0, 'neg': 0, 'neu': 1}\n            \n        text = text.lower()\n        words = re.findall(r'\\b\\w+\\b', text)  # Simple tokenization\n        \n        total_score = 0\n        word_count = 0\n        \n        for word in words:\n            if word in self.lexicon:\n                total_score += self.lexicon[word]\n                word_count += 1\n        \n        # Calculate compound score (normalized between -1 and 1)\n        compound = 0\n        if word_count > 0:\n            compound = total_score / (word_count + 0.1)  # Add small value to avoid division by zero\n            # Normalize between -1 and 1\n            compound = max(min(compound, 1.0), -1.0)\n            \n        # Return dict format similar to VADER\n        return {\n            'compound': compound,\n            'pos': max(0, compound),\n            'neg': max(0, -compound),\n            'neu': 1 - abs(compound)\n        }\n\n# Instantiate analyzer with your lexicon\nanalyzer = CustomVaderSentimentAnalyzer(lexicon_dict)\n\n# Step 5: Define sentiment labeling logic\ndef get_sentiment_label(score):\n    if score <= -0.05:\n        return 'Negative'\n    elif score >= 0.05:\n        return 'Positive'\n    else:\n        return 'Neutral'\n\n# Step 6: Fetch articles with updated column handling\ndef fetch_data_from_snowflake():\n    # Get all columns from the table\n    snowflake_table = session.table(\"FWDDEMO.PUBLIC.ARTICLECONTENTSCRAPED_MATCHED\")\n    df = snowflake_table.to_pandas()\n    \n    # Display the column names to help debug\n    #st.write(\"Columns in dataframe:\", df.columns.tolist())\n    return df\n\narticle_df = fetch_data_from_snowflake()\n\n# Use first string column for sentiment analysis if content_scraped doesn't exist\ndef find_text_column(df):\n    for col in df.columns:\n        # Check if column exists and contains string data\n        if df[col].dtype == 'object':\n            sample = df[col].iloc[0] if not df.empty else None\n            if isinstance(sample, str):\n                return col\n    return None\n\ntext_column = 'content_scraped' if 'content_scraped' in article_df.columns else find_text_column(article_df)\n\nif text_column:\n    st.write(f\"Using column '{text_column}' for sentiment analysis\")\n    \n    # Step 7: Apply sentiment scoring with error handling\n    def safe_sentiment_analysis(text):\n        try:\n            if pd.isna(text) or text is None or not isinstance(text, str):\n                return 0  # Neutral sentiment for invalid texts\n            return analyzer.polarity_scores(text)['compound']\n        except Exception as e:\n            st.warning(f\"Error processing text: {e}\")\n            return 0\n\n    # Apply sentiment scoring with error handling\n    article_df['article_sentiment'] = article_df[text_column].apply(safe_sentiment_analysis)\n    article_df['sentiment_label'] = article_df['article_sentiment'].apply(get_sentiment_label)\n\n    # --- Save back to Snowflake ---\n    snowpark_df = session.create_dataframe(article_df)\n    snowpark_df.write.mode(\"overwrite\").save_as_table(\"FWDDEMO.PUBLIC.ArticleContentScraped_Sentiment\")\n    \n    # Step 8: Display result\n    #st.dataframe(article_df[[text_column, 'article_sentiment', 'sentiment_label']])\n    st.dataframe(article_df)\nelse:\n    st.error(\"No suitable text column found for sentiment analysis\")",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d565816c-0bf1-4f6e-89c0-e379169bf904",
   "metadata": {
    "name": "Step5",
    "collapsed": false
   },
   "source": "### Alter column SP_500_Symbol from varchar to varant \n- Copy the code below into Snoflake worksheet"
  },
  {
   "cell_type": "code",
   "id": "ad7b391c-82d0-4725-af02-c9fd08f48a79",
   "metadata": {
    "language": "sql",
    "name": "Alter_SP_500_SYMBOL_Datatype"
   },
   "outputs": [],
   "source": "-- Alter column SP_500_Symbol from varchar to varant \nALTER TABLE FWDDEMO.PUBLIC.ARTICLECONTENTSCRAPED_SENTIMENT\nADD COLUMN SP_500_Symbol_variant VARIANT;\n\nUPDATE FWDDEMO.PUBLIC.ARTICLECONTENTSCRAPED_SENTIMENT\nSET SP_500_Symbol_variant = TO_VARIANT(\"SP_500_Symbol\");\n\nUPDATE FWDDEMO.PUBLIC.ARTICLECONTENTSCRAPED_SENTIMENT\nSET SP_500_Symbol_variant = PARSE_JSON(\"SP_500_Symbol\");\n\nALTER TABLE FWDDEMO.PUBLIC.ARTICLECONTENTSCRAPED_SENTIMENT\nDROP COLUMN \"SP_500_Symbol\";\n\nALTER TABLE FWDDEMO.PUBLIC.ARTICLECONTENTSCRAPED_SENTIMENT\nRENAME COLUMN SP_500_Symbol_variant TO SP_500_Symbol;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2345236f-f13c-40f5-8265-bec2f21b74c9",
   "metadata": {
    "name": "Step6",
    "collapsed": false
   },
   "source": "### Create a vew table for the SP_500_articles \n\n- This table consist of the new columns from fuzzy matching and sentiment analysis"
  },
  {
   "cell_type": "code",
   "id": "c32dff86-2a5a-4faf-a244-6eebd1bdfe9a",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "--create or replace view FWDDEMO.PUBLIC.\"SP_500_ARTICLES\" as\nselect \n    \"source\" as SOURCE, \n    \"title\" as ARTICLE_TITLE, \n    \"description\" as ARTICLE_SUMMARY, \n    \"url\" as ARTICLE_URL, \n    \"publishedAt\"::DATE as PUBLISH_DATE,\n    \"content_scraped\" as ARTICLE_CONTENT, \n    value::text AS TICKER,\n    \"article_sentiment\"::float as SENTIMENT_SCORE,\n    \"sentiment_label\" as SENTIMENT\nfrom\n  FWDDEMO.PUBLIC.ARTICLECONTENTSCRAPED_SENTIMENT src,\n  LATERAL FLATTEN (INPUT => \"SP_500_SYMBOL\") AS _flattened (SEQ, KEY, PATH, INDEX, VALUE, THIS);",
   "execution_count": null
  }
 ]
}
